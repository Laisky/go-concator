# 这里放一些常数项，方便后续配置时引用。
# 主要起文档作用，不会被直接使用
consts:
  ports:
    "18083": rsyslog recv
    "22800": http
    "24225": fluentd recv
    "24226": forward recv
  kafka:
    1-1-1-1-9092: &1-1-1-1-9092
      - 1.1.1.1:9092
    2-2-2-2-9092: &2-2-2-2-9092
      - 2.2.2.2:9092
  tags:
    all-tags: &all-tags
      - test
      - app.spring
      - gateway
      - connector
      - cp
      - ptdeployer
      - httpguard
      - ramjet
      - tsp
      - ai
      - base
      - geely
      - bot
      - spark
      - emqtt
      - kafkabuf
      - wechat
      - forward-wechat
      - fluentd-forward
  envs:
    all-env: &all-env
      - sit
      - perf
      - uat
      - prod


# settings 里是真正的会被程序使用到的配置
# 按照各个不同的组件进行拆分。
# 如果键名为 plugins，说明这部分配置是可以按需求进行增／删的
settings:
  # 针对 acceptor 组件的配置
  acceptor:
    # 传给 acceptPipeline 的两个出口，分别是同步阻塞的和非阻塞的
    async_out_chan_size: 100000
    sync_out_chan_size: 10000
    # 用来滚动 msgid 的值，随手敲了个大数
    max_rotate_id: 372036854775807  # do not change to smaller number
    # 配置 recv 插件，每一个 recv 都用来接受一种类型的数据
    recvs:
      plugins:


        # 解析 fluentd 协议的插件
        fluentd:
          # 指定插件类型
          type: fluentd
          # 对哪些环境生效
          active_env: *all-env
          # 消息体里内用来标记 tag 的 key
          tag_key: tag
          # 监听地址
          addr: 0.0.0.0:24225
          # 是否使用消息体内 tag 覆盖 msg.Tag
          is_rewrite_tag_from_tag_key: false


        # 同样是解析 fluentd 协议，不过这个接口专门用来处理由 go-fluetd-forward 转发而来的数据
        fluentd-forward:
          type: fluentd
          active_env: *all-env
          tag_key: tag
          addr: 0.0.0.0:24226
          # 需要使用消息体里的 tag 改写 msg.Tag
          is_rewrite_tag_from_tag_key: true


        # 解析 rsyslog 的 udp 数据，目前用于接受 emqtt 数据
        rsyslog:
          type: rsyslog
          active_env: *all-env
          tag_key: tag
          addr: 0.0.0.0:24514
          # 从 rsyslog 中加载时间戳
          time_key: timestamp
          msg_key: content
          # 重命名时间戳（和其他日志保持统一）
          new_time_key: "@timestamp"
          # 修改时间格式（和其他日志保持统一）
          new_time_format: "2006-01-02T15:04:05.000Z"


        # 从 kafka 拉取数据，目前仅用于 kafka
        spark:
          type: kafka
          active_env: *all-env
          interval_num: 5000
          interval_sec: 3
          nconsumer: 6
          msg_key: log
          tag_key: tag
          is_json_format: false
          # meta 用来给消息体添加新的键值对
          meta:
            # 添加 container_id，因为后续的很多流程都会用到
            container_id: "go-fluentd"
          groups:
            sit: paas_spark_log_sit_v2
            uat: paas_spark_log_uat_v2
            perf: paas_spark_log_perf_v2
            prod: paas_spark_log_prod_v2
          topics:
            sit: logaiSit
            uat: logaiUat
            perf: logaiPerf
            prod: logaiPro
          tags:
            sit: spark.sit
            uat: spark.uat
            perf: spark.perf
            prod: spark.prod
          brokers:
            sit: *1-1-1-1-9092
            uat: *1-1-1-1-9092
            perf: *1-1-1-1-9092
            prod: *2-2-2-2-9092


  # producer 组件的配置
  producer:
    discard_chan_size: 50000   # 用于提交已发送的 msgid
    sender_inchan_size: 50000  # 入队缓冲
    # senders 插件的配置
    # support types: fluentd/kafka/es
    plugins:


      # 给 ES 发送 HTTP 的 sender
      es_general:
        type: es
        active_env: *all-env
        # 支持的 tags
        tags:
          # - es-general
          - test
          - app.spring
          - gateway
          - connector
          - cp
          - ptdeployer
          - fluentd-forward
          - httpguard
          - ramjet
          - tsp
          - ai
          - base
          - bot
          - spark
          - emqtt
          - kafkabuf
          - wechat
        forks: 3
        # 目标 es 地址
        addr: http://es/_bulk
        # 发送的契机，每 1000 条发送一次，每 3 秒发送一次
        msg_batch_size: 1000
        max_wait_sec: 3
        tag_key: tag
        # 发送失败后，重试队列的长度
        retry_chan_len: 10000
        # tag->index 的对应关系，因为不同 tag 会存到不同的 index 里
        indices:
          ramjet.{env}: "{env}-spring-logs-write"
          httpguard.{env}: "{env}-spring-logs-write"
          fluentd-forward.{env}: "{env}-spring-logs-write"
          app.spring.{env}: "{env}-spring-logs-write"
          gateway.{env}: "{env}-gateway-logs-write"
          connector.{env}: "{env}-connector-logs-write"
          cp.{env}: "{env}-cp-logs-write"
          ptdeployer.{env}: "{env}-spring-logs-write"
          tsp.{env}: "{env}-spring-logs-write"
          ai.{env}: "{env}-spring-logs-write"
          base.{env}: "{env}-spring-logs-write"
          bot.{env}: "{env}-spring-logs-write"
          spark.{env}: "{env}-spark-logs-write"
          emqtt.{env}: "{env}-emqtt-logs-write"
          wechat.{env}: "{env}-wechat-logs-write"
        # 当 sender 处理不过来时，是否要丢弃数据。
        # 一般来说不应该，因为丢弃数据会导致该 tag 对应的本批次 sender 全部重试。
        is_discard_when_blocked: false


      # 也是 es 的sender，不过是负责处理 geely 的，
      # 因为 geely 的节点独立，所以需要指向不同的 addr
      es_geely_v2:
        type: es
        active_env: *all-env
        tags:
          # - es-geely
          - geely
        forks: 10
        addr: http://es2/_bulk
        msg_batch_size: 1000
        max_wait_sec: 3
        tag_key: tag
        retry_chan_len: 10000
        indices:
          geely.{env}: "{env}-geely-logs-write"
        is_discard_when_blocked: false


      # kafka 的 sender，因为 cp 的日志需要往大数据的 kafka 发送一份
      kafka_cp:
        type: kafka
        active_env: *all-env
        tag_key: tag
        brokers:
          sit: *1-1-1-1-9092
          perf: *1-1-1-1-9092
          uat: *1-1-1-1-9092
          prod: *2-2-2-2-9092
        topic: docker_message
        tags:
          - cp
        forks: 3
        msg_batch_size: 10000
        max_wait_sec: 5
        retry_chan_len: 100000
        is_discard_when_blocked: false


      # fluentd 协议的 sender，将日志发送给 backup 服务器做冷备份
      fluentd_backup_geely:
        type: fluentd
        active_env:
          - prod
        tags:
          - geely.prod
        forks: 3
        addr: fluentd-sit.ptcloud.t.home:24235
        msg_batch_size: 10000
        max_wait_sec: 5
        retry_chan_len: 50000
        is_discard_when_blocked: true
      fluentd_backup_emqtt:
        type: fluentd
        active_env:
          - prod
        tags:
          - emqtt.prod
        forks: 3
        addr: fluentd-sit.ptcloud.t.home:24236
        msg_batch_size: 10000
        max_wait_sec: 5
        retry_chan_len: 50000
        is_discard_when_blocked: true


  # journal 组件，负责在磁盘上对所有日志做持久化，以保证 at-least-once 的日志处理
  journal:
    # 持久化文件所在路径
    buf_dir_path: /data/log/fluentd/go-concator
    # 每一个持久化文件的大小
    buf_file_bytes: 209315200  # 200 MB
    # 缓冲区
    journal_out_chan_len: 100000
    commit_id_chan_len: 500000


  # acceptorPipeline 组件，负责在 recvs 收到数据后，做一些很简单的前处理
  acceptor_filters:
    out_buf_len: 150000
    reenter_chan_len: 5000
    fork: 4
    # 是否限速，以及一些限速的配置（使用令牌法）
    is_throttle: false
    # 允许的瞬间 burst 值
    throttle_max: 10000
    # 每秒新加的令牌数
    throttle_per_sec: 5000
    plugins:


      # 对 spark 日志的前处理
      # 简单的忽略一些不符合格式的日志
      spark:
        type: spark
        msg_key: log
        identifier: container_id
        ignore_regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2},\d{3} +\[


      # 对 spring 日志的前处理
      # 因为 cp、baidubot 和普通应用日志都使用的 spring 作为 tag，所以需要根据日志格式对其分流到不同的 tag
      spring:
        type: spring
        msg_key: log
        tag_key: tag
        rules:
          - new_tag: cp.{env}
            regexp: ms:cp
          - new_tag: bot.{env}
            regexp: ms_\w+\ {0,}\|
          - new_tag: app.spring.{env}
            regexp: .*


  # postPipeline 组件，处在 tagPipeline 到 senders
  post_filters:
    reenter_chan_len: 10000
    out_chan_size: 50000
    fork: 4
    plugins:
      # 默认插件，限制日志的最大长度
      default:
        msg_key: log
        max_len: 307200


      # 重写 msg.Tag="forward-wechat" 的 tag 为 msg.Message[tag_key]
      forward_tag_rewriter:
        type: tag-rewriter
        tag_key: tag
        tag: forward-wechat


  # dispatcher 组件，这个组件在 acceptorFilters 之后，负责把 msg 分发给 tagFilters
  dispatcher:
    nfork: 2
    out_chan_size: 10000


  # tag_filters 最重要的职责就是拼接和解析。
  tag_filters:
    internal_chan_size: 10000
    plugins:


      # 解析 golang 应用的日志
      go-apps:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - fluentd-forward
          - httpguard
          - ramjet
        # 是否需要对消息体内的某些字段应用 json 解析
        parse_json_key: log
        # 对日志时间的处理
        # 读取时间字段和格式
        time_key: ts
        time_format: "2006-01-02T15:04:05.000-0700"
        # 改写为新的日期格式
        new_time_format: "2006-01-02T15:04:05.000000Z"
        # 是否保留原日期字段
        reserved_time_key: false
        # 新的日期字段
        new_time_key: "@timestamp"
        # 添加新的字段
        # 为指定的 tag 添加新的 key: value
        add:  # optional
          fluentd-forward:
            datasource: fluentd-forward
          httpguard:
            datasource: httpguard
          ramjet:
            datasource: ramjet


      # 解析 connector 的日志
      connector:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - connector
          - gateway
        # 日志所在的字段
        msg_key: log
        # 2018-04-02 02:02:10.928 | sh-datamining | INFO | http-nio-8080-exec-80 | com.pateo.qingcloud.gateway.core.zuul.filters.post.LogFilter | 74 | {"key": "value"}: xxx
        # 2018-04-02 02:02:10.928 | sh-datamining | INFO | http-nio-8080-exec-80 | com.pateo.qingcloud.gateway.core.zuul.filters.post.LogFilter | 74 | xxx
        # 2018-03-12 02:02:10.928 | gateway | INFO | http-nio-8080-exec-80 | com.pateo.qingcloud.gateway.core.zuul.filters.post.LogFilter | 74: {"key": "value"}:xxx
        # 2019-02-18 15:42:06.635 | usertracking | DEBUG | kafka-producer-network-thread | producer-1 | com.pateo.qingcloud.base.usertracking.service.KafkaServiceBase$ProducerCallBack | 63: onCompletion when sendMessage to topic:UserTracking,partition:0  success!!
        # 使用正则解析
        pattern: (?ms)^(?P<time>.{23}) {0,}\| {0,}(?P<app>[^\|]+) {0,}\| {0,}(?P<level>[^\|]+) {0,}\| {0,}(?P<thread>[^\|]+) {0,}\| {0,}((?P<producer>[\w\d\-]+) {0,}\| {0,}){0,1}(?P<class>[^\|]+) {0,}\| {0,}(?P<line>\d+) {0,}([\|:] {0,}(?P<args>\{.*\}))?([\|:] {0,}(?P<message>.*))?
        is_remove_orig_log: true
        parse_json_key: args
        # 解析后必须包含该此段，否则就丢弃
        must_include: app
        # ⬇⬇ time
        time_key: time
        time_format: "2006-01-02 15:04:05.000 -0700"
        new_time_format: "2006-01-02T15:04:05.000000Z"
        reserved_time_key: false
        new_time_key: "@timestamp"
        # 解析日期前，先为原日期字符串添加上时区信息
        append_time_zone:
          sit: "+0800"
          perf: "+0800"
          uat: "+0800"
          prod: "+0800"


      # 解析 geely 的日志格式
      geely:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - geely
        msg_key: log
        # 2018-02-05 10:33:13.408 | geely:nlcc | INFO | http-bio-8081-exec-3 | com.tservice.cc.web.interceptor.MyLoggingOutInterceptor.handleMessage:57 - Outbound Message:{ID:1, Address:http://10.133.200.77:8082/gisnavi/tservice/gisnavi/poi/poicategory, Http-Method:GET, Content-Type:application/json, Headers:{Content-Type=[application/json], Accept=[application/json]}}
        pattern: (?ms)^(?P<time>.{23}) {0,}\| {0,}(?P<project>[^\|]+) {0,}\| {0,}(?P<level>[^\|]+) {0,}\| {0,}(?P<thread>[^\|]+) {0,}\| {0,}(?P<class>[^\:]+)\:(?P<line>\d+) {0,}- {0,}(?P<message>.+)
        is_remove_orig_log: true
        must_include: project
        # ⬇⬇ time
        time_key: time
        time_format: "2006-01-02 15:04:05.000 -0700"
        new_time_format: "2006-01-02T15:04:05.000Z"
        reserved_time_key: false
        new_time_key: "@timestamp"
        append_time_zone:
          sit: "+0800"
          perf: "+0800"
          uat: "+0800"
          prod: "+0800"


      # 解析 emqtt 的日志
      emqtt:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - emqtt
        add:
          emqtt:
            datasource: emqtt


      # 解析 spark 的日志
      spark:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - spark
        msg_key: log
        # 2018/03/06 15:19:23.619 INFO 6356e435e4894a22a41c80e6ade35526 com.pateo.qingcloud.ai.sparkstreaming.db.AiDbService: Enter getFuseDataType  function!
        pattern: "(?ms)^(?P<time>.{23}) {0,}(?P<level>[^ ]+) +(?P<uuid>[^ ]+) +(?P<app_info>[^:]+) {0,}: {0,}(?P<message>.*)"
        is_remove_orig_log: true
        must_include: app_info
        # ⬇⬇ time
        time_key: time
        time_format: "2006/01/02 15:04:05.000 -0700"
        new_time_format: "2006-01-02T15:04:05.000000Z"
        reserved_time_key: false
        new_time_key: "@timestamp"
        append_time_zone:
          sit: "+0800"
          perf: "+0000"
          uat: "+0000"
          prod: "+0800"


      # 解析 ptdeployer 的日志
      ptdeployer:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - ptdeployer
        msg_key: log
        # [2018-11-12 03:41:39,735 - ptdeployer - views.py - 1429 - INFO - app_log] - zihengni登录成功
        pattern: (?ms)^\[(?P<time>.{23}) {0,}- {0,}(?P<app>[^\-]+) {0,}- {0,}(?P<class>[^\-]+) {0,}- {0,}(?P<line>\d+) {0,}- {0,}(?P<level>[^\-]+) {0,}- {0,}(?P<thread>[^\|]+)\] {0,}- {0,}(?P<message>.*)
        is_remove_orig_log: true
        must_include: app
        add:  # optional
          ptdeployer:
            datasource: ptdeployer
        # ⬇⬇ time
        time_key: time
        time_format: "2006-01-02 15:04:05,000 -0700"
        new_time_format: "2006-01-02T15:04:05.000000Z"
        reserved_time_key: false
        new_time_key: "@timestamp"
        append_time_zone:
          sit: "+0800"
          perf: "+0800"
          uat: "+0800"
          prod: "+0800"


      # 解析 cp 的日志
      cp:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - cp
        msg_key: log
        # 2018-02-01 16:15:43.518 - ms:cp|type:platform|uuid:4f99962d-c272-43bb-85d9-20ab030180b7|dateTime:2018-02-01 16:15:43.518|customerSid:27|customerCode:DT00000000|customerName:默认
        pattern: (?ms)^(?P<time>.{23}) - (?P<message>ms:cp.*)
        is_remove_orig_log: true
        must_include: message
        add:  # optional
          cp:
            datasource: cp
        # ⬇⬇ time
        time_key: time
        time_format: "2006-01-02 15:04:05.000 -0700"
        new_time_format: "2006-01-02T15:04:05.000000Z"
        reserved_time_key: false
        new_time_key: "@timestamp"
        append_time_zone:
          sit: "+0800"
          perf: "+0800"
          uat: "+0800"
          prod: "+0800"


      # 解析 spring 的日志
      spring:
        type: parser
        lb_key: container_id
        nfork: 3
        tags:
          - app.spring
          - ai
          - tsp
          - base
          - bot
        msg_key: log
        # 2018-03-06 16:56:22.514 | mscparea | INFO  | http-nio-8080-exec-1 | com.pateo.qingcloud.cp.core.service.impl.CPBusiness.reflectAdapterRequest | 84: test
        pattern: (?ms)^(?P<time>.{23}) {0,}\| {0,}(?P<app>[^\|]+) {0,}\| {0,}(?P<level>[^\|]+) {0,}\| {0,}(?P<thread>[^\|]+) {0,}\| {0,}(?P<class>[^ ]+) {0,}\| {0,}(?P<line>\d+) {0,}([\|:] {0,}(?P<args>\{.*\}))?([\|:] {0,}(?P<message>.*))?
        is_remove_orig_log: true
        must_include: app
        # ⬇⬇ time
        time_key: time
        time_format: "2006-01-02 15:04:05.000 -0700"
        new_time_format: "2006-01-02T15:04:05.000000Z"
        reserved_time_key: false
        new_time_key: "@timestamp"
        append_time_zone:
          sit: "+0800"
          perf: "+0800"
          uat: "+0800"
          prod: "+0800"
        # ⬇⬇ fields
        add:  # optional
          ai:
            datasource: ai
          tsp:
            datasource: tsp
          base:
            datasource: base
          bot:
            datasource: bot
          app.spring:
            datasource: spring


      # concator 插件，负责日志拼接
      concator:
        type: concator
        config:
          max_length: 100000
          lb_key: container_id
          nfork: 3
        plugins:
          test:
            msg_key: log
            identifier: container_id
            regex: ^\d{4}-\d{2}-\d{2}

          # 拼接 spark 日志
          spark:
            msg_key: log
            # 相同 identifier 的，会被拼接在一起
            identifier: container_id
            # 首行匹配的正则，不符合匹配规则的消息会被拼接到上一行日志后面。
            regex: ^\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2}.\d{3} \w+
          cp:
            msg_key: log
            identifier: container_id
            # 2018-02-01 16:15:43.518 - ms:cp|type:platform|uuid:4f99962d-c272-43bb-85d9-20ab030180b7|dateTime:2018-02-01 16:15:43.518|customerSid:27|customerCode:DT00000000|customerName:默认
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}.\d{3} - ms:cp\|
          ramjet:
            msg_key: log
            identifier: container_id
            # [2018-08-03T05:42:42.714151Z - DEBUG - /go/src/github.com/Laisky/go-ramjet/tasks/logrotate/backup/main.go:129] IsFileReadyToUpload for 2018050700.log.gz
            regex: ^\[\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{6}Z - \w+ -
          app.spring:
            msg_key: log
            identifier: container_id
            # 2018-03-06 16:56:22.514 | mscparea | INFO  | http-nio-8080-exec-1 | com.pateo.qingcloud.cp.core.service.impl.CPBusiness.reflectAdapterRequest | 84: test
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}\.\d{3} {0,}\|
          tsp:
            msg_key: log
            identifier: container_id
            # 2018-03-06 16:56:22.514 | mscparea | INFO  | http-nio-8080-exec-1 | com.pateo.qingcloud.cp.core.service.impl.CPBusiness.reflectAdapterRequest | 84: test
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}\.\d{3} {0,}\|
          ai:
            msg_key: log
            identifier: container_id
            # 2018-03-06 16:56:22.514 | mscparea | INFO  | http-nio-8080-exec-1 | com.pateo.qingcloud.cp.core.service.impl.CPBusiness.reflectAdapterRequest | 84: test
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}\.\d{3} {0,}\|
          base:
            msg_key: log
            identifier: container_id
            # 2018-03-06 16:56:22.514 | mscparea | INFO  | http-nio-8080-exec-1 | com.pateo.qingcloud.cp.core.service.impl.CPBusiness.reflectAdapterRequest | 84: test
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}\.\d{3} {0,}\|
          bot:
            msg_key: log
            identifier: container_id
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}\.\d{3} {0,}\|
          gateway:
            msg_key: log
            identifier: container_id
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}\.\d{3} {0,}\|
          connector:
            msg_key: log
            identifier: container_id
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}\.\d{3} {0,}\|
          geely:
            msg_key: log
            # 2018-02-05 10:33:13.408 | geely:nlcc | INFO | http-bio-8081-exec-3 | com.tservice.cc.web.interceptor.MyLoggingOutInterceptor.handleMessage:57 - Outbound Message:{ID:1, Address:http://10.133.200.77:8082/gisnavi/tservice/gisnavi/poi/poicategory, Http-Method:GET, Content-Type:application/json, Headers:{Content-Type=[application/json], Accept=[application/json]}}
            # 2018-04-15 10:59:31.096 | geely:vca | INFO  | I/O dispatcher 2 | MONITORLOG.writeMonitorLog:26 - 2018-04-15 10:59:31.096|VCA|XXX19YYYY10040463|vca00157f9c04ff887f2b3488ddc4f2b|VCA_TRIGGER_OUT'
            identifier: container_id
            regex: ^\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}.\d{3} +\|
          ptdeployer:
            msg_key: log
            identifier: container_id
            regex: ^\[\d{4}-\d{2}-\d{2} +\d{2}:\d{2}:\d{2}.\d{3} +-
